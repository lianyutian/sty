# HDFS简介

HDFS（Hadoop Distributed File System）是 Apache Hadoop 生态系统中的一部分，是一个开源的分布式文件系统。它的设计是为了能够在大规模集群上存储和处理海量数据，具有高容错性、高可靠性和高可扩展性等特点。

HDFS 的架构采用了主从结构，其中一个 NameNode 负责管理整个文件系统的命名空间和文件的元数据信息，多个 DataNode 负责存储数据块并提供读写服务。HDFS 的数据块默认大小为 128MB，这种较大的块大小可以提高系统的读写性能。

HDFS 的主要优势在于它可以运行在廉价的硬件上，并且可以自动处理硬件故障和数据复制等问题。此外，HDFS 还支持对数据进行快速的随机读写操作，并且可以与 Hadoop 的其他组件如 MapReduce、YARN 和 Hive 等配合使用，提供一个完整的数据处理解决方案。

需要注意的是，由于 HDFS 的数据存储是分散在多个节点上的，所以在进行数据操作时需要考虑数据的分布情况，避免数据倾斜和网络拥塞等问题。

# HDFS优缺点

HDFS优点
------

*   **高可靠性**：HDFS 使用多副本机制存储数据，可以在硬件故障等情况下自动恢复，提供了高可靠性的数据存储解决方案。
*   **高扩展性**：HDFS 可以运行在大规模集群上，可以方便地扩展存储容量和计算能力，适合处理大规模数据。
*   **高容错性**：HDFS 采用主从结构，其中 NameNode 可以容忍单点故障，并且可以进行备份和恢复，提高了系统的容错性。
*   **高吞吐量**：HDFS 采用较大的数据块大小和数据本地化机制，可以提供高吞吐量的数据读写服务。

HDFS缺点
------

* **不适合小文件存储**：

  - 存储大量小文件的话，会占用Name Node大量的内存来存储文件目录和块信息。
  - 小文件存储的寻址时间可能会超过读取时间，违背了HDFS的设计目标。

  - 由于 HDFS 的块大小默认为 128MB，所以对于小文件存储会造成空间浪费和读写性能下降的问题。

* **不支持并发写入**：HDFS 不支持多个客户端同时写入同一个文件，需要进行文件加锁等操作来保证数据的一致性。

* **不适合低延迟数据访问**：由于 HDFS 主要面向批处理数据处理场景，对于低延迟数据访问的支持不够好。

*   **需要专门的管理和维护**：HDFS 部署和维护需要专门的人员和技术支持，对于小规模的数据存储需求不够友好。

# HDFS组成架构

![](img\HDFS架构图.png)

1.  **NameNode（名称节点）**：NameNode 是 HDFS 架构的中心组件，它负责管理整个文件系统的命名空间和文件的元数据信息。NameNode 存储了每个文件和目录的信息，包括文件的名称、块的位置、块的大小和复制因子等。NameNode 还协调客户端的读写请求，并且负责备份和恢复元数据信息，以提高系统的容错性。
2.  **DataNode（数据节点）**：DataNode 是 HDFS 存储集群中的工作节点，它负责存储数据块并提供读写服务。每个数据块在存储时会被复制多份，通常是三份，以提高数据的可靠性和容错性。DataNode 还会向 NameNode 报告自己所存储的块的信息，以帮助 NameNode 维护文件的元数据信息。
3.  **客户端**：客户端是 HDFS 文件系统的用户，它通过 HDFS API 与 NameNode 和 DataNode 进行交互，进行文件的读写操作。客户端会向 NameNode 请求文件的元数据信息，然后与 DataNode 直接进行数据的读写操作。在进行数据读写时，客户端可以根据数据块的位置和复制因子等信息来选择最优的 DataNode 进行操作，以提高数据读写的效率。

除了上述三个核心组件外，HDFS 还包括一些辅助组件，如 SecondaryNameNode、CheckpointNode 和 BackupNode 等，它们的作用是帮助提高系统的容错性和可靠性。

# HDFS文件块大小

HDFS 采用较大的文件块大小（默认为 128MB）的设计，主要是为了提高数据本地性和 I/O 性能。

以下是一些原因：

1. **磁盘传输速率**：如果寻址时间约为10ms，即查找到目标block的时间为10ms。寻址时间为传输时间的1%时为最佳。因此，传输时间=10ms/0.01=1000ms=1s。而目前磁盘的传输速率普遍在100MB/s。
2. **数据本地性**：HDFS 能够将数据分散存储在多台计算机上，因此可以充分利用计算集群中的计算资源。然而，如果要从远程节点读取数据，那么将会产生较高的网络开销。因此，通过将大量的数据存储在本地节点上，可以减少数据传输的网络开销，从而提高数据读取的性能。
3. **I/O 性能**：较大的文件块大小可以带来更好的磁盘读取和写入性能。这是因为较大的文件块可以在一次读写操作中传输更多的数据，从而减少了磁盘寻址和数据传输的次数。这可以有效地减少 I/O 操作的时间和磁盘磨损，同时提高了整个系统的吞吐量。
4. **减少 NameNode 压力**：较大的文件块大小可以减少 NameNode 的压力，因为在默认情况下，每个文件块都需要在 NameNode 中存储一条记录。因此，较大的文件块大小可以减少存储在 NameNode 中的记录数目，从而减轻 NameNode 的负担。

然而，使用较大的文件块大小也会带来一些缺点，比如存储小文件时的空间浪费和读取小文件时的效率问题。因此，在使用 HDFS 时，需要根据实际情况进行合理的块大小设置。

# HDFS常用shell命令

以下是 HDFS 常用的一些 Shell 命令：

1.  `hadoop fs -ls [path]`：列出指定路径下的文件和文件夹信息。
    
2.  `hadoop fs -mkdir [dir]`：创建指定路径的文件夹。
    
3.  `hadoop fs -put [src] [dst]`：将本地文件复制到 HDFS 中。
    
4.  `hadoop fs -get [src] [dst]`：将 HDFS 中的文件复制到本地。
    
5.  `hadoop fs -cat [file]`：输出 HDFS 文件的内容。
    
6.  `hadoop fs -mv [src] [dst]`：将 HDFS 中的文件或文件夹移动到指定路径。
    
7.  `hadoop fs -rm [path]`：删除指定路径的文件或文件夹。
    
8.  `hadoop fs -du [path]`：显示指定路径的大小信息。
    
9.  `hadoop fs -chmod [mode] [path]`：改变指定路径的权限。
    
10.  `hadoop fs -chown [owner:group] [path]`：改变指定路径的所有者和组。
    
11.  `hadoop fs -tail [file]`：输出 HDFS 文件的末尾几行。
    
12.  `hadoop fs -count [path]`：统计指定路径下文件和文件夹的数量和总大小。

这些命令可以通过在命令行中输入 `hadoop fs -help` 获取更详细的信息和用法。同时，还可以通过在命令行中输入 `hadoop fs -usage` 获取一些示例。

# HDFS常用API

HDFS 的 Java API 是 Hadoop 生态系统中最常用的 API 之一，提供了完整的访问 HDFS 的接口，包括文件系统操作、文件 I/O、权限管理等功能。以下是 HDFS 常用的 Java API：

1. **FileSystem API**：`org.apache.hadoop.fs.FileSystem` 类提供了访问 HDFS 的核心接口，包括文件系统操作、文件 I/O、权限管理等功能。可以使用该类创建文件系统对象，然后通过文件系统对象进行文件操作。
2. **Path API**：`org.apache.hadoop.fs.Path` 类用于表示 HDFS 中的路径，可以用于创建文件路径、检查路径是否存在、比较路径等操作。
3. **FSDataInputStream 和 FSDataOutputStream API**：`org.apache.hadoop.fs.FSDataInputStream` 和 `org.apache.hadoop.fs.FSDataOutputStream` 类提供了访问 HDFS 中文件的输入和输出流，可以用于读写 HDFS 文件。
4. **FileSystemPermission API**：`org.apache.hadoop.fs.permission.FileSystemPermission` 类用于表示 HDFS 文件的权限信息，可以用于创建文件时设置文件权限。
5. **FileSystem.Statistics API**：`org.apache.hadoop.fs.FileSystem.Statistics` 类用于表示 HDFS 文件系统的统计信息，可以用于获取文件系统的读写操作次数、耗时等信息。

以上是 HDFS 常用的 Java API，可以根据具体的使用场景进行选择。此外，Hadoop 还提供了其他一些 Java API，如 MapReduce API、YARN API 等，用于开发 Hadoop 应用程序。

# HDFS写入流程

HDFS 写数据的流程如下：

1.  客户端通过 Hadoop API 打开要写入的文件，并将数据分成若干个数据块。
    
2.  客户端向 NameNode 发送创建文件请求，包括文件名、块大小、副本数等信息。
    
3.  NameNode 接收到创建文件请求后，首先会检查文件是否存在，如果不存在，则创建文件元数据信息，包括文件名、块信息、副本信息等，并将其存储到内存中和编辑日志中。同时，NameNode 也会为每个数据块选择一个或多个 DataNode，作为数据块的副本位置，并将这些信息写入文件元数据信息中。
    
4.  NameNode 返回创建成功的响应给客户端，客户端根据文件元数据信息将数据块分配到不同的 DataNode 上，并将数据块的副本数写入元数据信息中。
    
5.  客户端向每个 DataNode 发送写数据请求，并将数据块发送到对应的 DataNode。客户端可以根据数据块的副本位置信息，选择其中一个 DataNode 作为主 DataNode，向其发送写请求，其他 DataNode 则作为副本 DataNode，接收主 DataNode 发送的数据，并将其写入本地磁盘中。
    
6.  每个 DataNode 接收到数据块后，先将数据块写入本地磁盘中，同时向主 DataNode 发送复制请求，请求将该数据块的副本复制到其他副本 DataNode 上。主 DataNode 收到复制请求后，将数据块的副本复制到其他副本 DataNode 上，并向副本 DataNode 发送复制成功的响应。
    
7.  当所有数据块的所有副本都成功写入到 DataNode 上后，客户端向 NameNode 发送关闭文件请求，NameNode 更新文件元数据信息中的修改时间和文件大小等信息，并将其存储到内存中和编辑日志中。
    
8.  如果发生了 DataNode 宕机等故障，NameNode 可以从文件元数据信息中获取数据块的副本位置信息，从而找到其他 DataNode 上的副本，确保数据的可靠性和高可用性。
    
9.  文件写入完成后，客户端关闭文件。
    

 HDFS 写数据涉及到多个节点的交互，需要进行多次网络传输和数据复制等操作，确保数据的可靠性和高可用性。

# HDFS读数据流程

HDFS 读数据的流程如下：

1. 客户端通过 Hadoop API 打开要读取的文件，并向 NameNode 发送读取文件请求。

2. NameNode 接收到读取文件请求后，会返回文件元数据信息给客户端，包括文件名、块信息、副本信息等。

3. 客户端根据文件元数据信息，选择一个 DataNode 作为首选的读取节点，并向其发送读取数据请求。

   - 在 HDFS 读数据的流程中，客户端需要选择一个 DataNode 作为首选的读取节点，通常采用以下两种策略：

     1.  就近原则：选择距离客户端最近的 DataNode 作为首选的读取节点。这种策略可以减少数据传输的延迟，提高读取速度，但可能会导致一些 DataNode 上的副本没有被充分利用，而其他 DataNode 上的副本被频繁读取，造成负载不均衡。

     2.  负载均衡原则：选择副本分布较均匀的 DataNode 作为首选的读取节点。这种策略可以平衡 DataNode 上的负载，提高系统的稳定性和可用性，但可能会导致数据传输的延迟较高，读取速度较慢。

     具体选择哪种策略，取决于应用场景和数据访问模式。在 Hadoop 中，可以通过配置参数`dfs.client.use.datanode.hostname`来指定选择 DataNode 的策略，默认使用就近原则。

4. DataNode 接收到读取数据请求后，先检查该 DataNode 上是否有该数据块的本地副本，如果有，则直接从本地磁盘中读取数据块；如果没有，则会向其他副本 DataNode 发送复制请求，请求将该数据块的副本复制到本地磁盘上，并在本地磁盘中读取数据块。

5. DataNode 将数据块发送给客户端，并向客户端发送确认消息。

6. 如果首选的 DataNode 发生故障或网络异常，客户端可以根据文件元数据信息中的副本位置信息，选择其他 DataNode 上的副本进行读取。

7. 客户端读取完所有数据块后，关闭文件。


* * *

# NameNode工作机制

**思考：NameNode中的元数据是存储在哪里的？**

首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。

这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。

但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节SecondaryNamenode，专门用于FsImage和Edits的合并。

因此，当 NameNode 重新启动时，它会先读取磁盘上的 FsImage 文件，然后再读取 EditLog 文件中的操作日志，以恢复内存中的元数据。这个过程通常称为 NameNode 的启动恢复过程。

![](img\NameNode工作机制.png)

# DataNode工作机制

在 HDFS 中，DataNode 是负责存储文件块数据的节点，它们通常运行在集群的数据节点上。下面是 DataNode 的工作机制：

1. 存储数据块：当一个文件被分割成若干个数据块后，每个数据块会被复制到多个 DataNode 上，以实现数据的冗余备份。DataNode 会把数据块存储在本地磁盘上，并定期向 NameNode 汇报数据块的状态信息。一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。

2. 处理客户端请求：当客户端需要读取或者写入一个数据块时，它会向 NameNode 发送请求，并得到 DataNode 的地址列表。客户端会选择最近的 DataNode 进行数据访问，以减少数据传输的延迟。

3. 数据传输：DataNode 之间会通过网络互相传输数据块，以保证数据的冗余备份。当一个 DataNode 上的数据块损坏或丢失时，它会从其他 DataNode 上复制一份数据块，并进行自我修复。

4. 处理心跳和块报告：每个 DataNode 会定期向 NameNode 发送心跳和块报告，以告诉 NameNode 自己的存储状态和可用空间。这些信息可以帮助 NameNode 识别存储故障和节点故障，并及时进行处理。

   ![](img\DataNode工作机制.png)

5. 进行数据块删除：当一个文件被删除时，NameNode 会通知 DataNode 删除相应的数据块。为了防止误删除和数据丢失，DataNode 会在删除数据块之前进行确认，并通知 NameNode 删除成功。